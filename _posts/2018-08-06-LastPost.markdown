---
layout: post
title:  "Final Blog Post"
date:   2018-08-13
---

Now that we have finally arrived at the end of coding period, I want to give you an overview of the whole project: what was the initial motivation and plan; what has been done; and what still would need to be implemented.

# Code & documentation
Over the summer I worked on the [IRAM.jl](https://github.com/haampie/IRAM.jl) repository which aims at implementing ARPACK's `eigs` function in pure Julia for nonsymmetric matrices. The documentation of the method can be found [here](https://haampie.github.io/IRAM.jl/latest/).

# Original plan
The aim was to implement a native Julia solver for the eigenvalue problem

```math
Ax = λx.
```
In particular, the aim was to solve this problem for large and sparse, nonsymmetric matrices which would replace ARPACK's `eigs` function. This helps reduce the dependency of Julia language on ARPACK. I will provide benchmarks comparing the performance of the native `eigs` versus ARPACK's `eigs`. The aim at first was to get the method into the package IterativeSolvers.jl. However, it is also a possibility to just register the package separately.

# Initial project outline
* IRAM for real and complex arithmetics with QR iterations through Givens rotations. __DONE__
* Finding multiple eigenpairs which requires locking of converged Ritz vectors. __DONE__
* Adding search targets for eigenvalues. __DONE__
	- However, search targets are not implemented in the same way as in ARPACK. The method targets the eigenvalues that are largest in magnitude by default which has the highest convergence rate. If the user wants to target the eigenvalues that are the smallest in magnitude, they have to invert the problem as detailed in the [documentation](https://haampie.github.io/IRAM.jl/latest/theory/transformations.html#Targeting-eigenvalues-with-shift-and-invert-1).
* Adding support for generalized eigenvalue problems.
	- This functionality is not incorporated in the method either. Generalized eigenvalue problems can be solved if the user provides an input matrix that contains the effect of both matrices in the generalized eigenvalue problem. A detailed explanation can be found in the [documentation](https://haampie.github.io/IRAM.jl/latest/theory/transformations.html#Transformation-to-standard-form-for-non-singular-B-1). 

__In addition:__
* Making the implementation fully native which includes native Julia implementations for the `schurfact!` call and the `eigvals` call. __DONE__

# What's left to do?

__Method-wise:__
* The criterion for convergence should be changed to how it is implemented in `eigs`. `eigs` considers a Ritz value `θ` converged when its residual `||Av - vθ||` is smaller than the product of `tol` and `max(ɛ^{2/3}, |θ|)`, where `ɛ = eps(real(eltype(A)))/2` is LAPACK's machine epsilon. Native `partial_schur` considers the Ritz value converged only when the subdiagonal of the Hessenberg matrix in smaller than the user provided tolerance `tol`. 

__Functionality-wise:__
* `partial_schur` returns the partial Schur decomposition. What still needs to be done is to compute the eigenvectors and eigenvalues from the partial Schur decomposition. Computing eigenvalues is quite straightforward and is currently implemented in `eigvalues`. However, computing the eigenvectors requires taking into account some stability considerations that are not currently implemented. Hence, currently the implementation only returns the partial Schur decomposition.

__Performance-wise & stability-wise:__
* The code needs to be profiled in order to find areas of improvement. The method should closely follow the implementation of `eigs`, however sometimes native `partial_schur` seems to be much slower than `eigs`. In fact, in some cases the native `partial_schur` requires more iterations than `eigs` to find the eigenvalues of the same matrix which suggests that there are differences between the methods themselves. A likely explanation for this is differences in the stopping criterion.
* Currently known performance considerations
	- Temporary variables could be refactored in order to reduce the amount of allocations
	- In `backward_subst!`, the case when the method computes eigenvectors for conjugate eigenvalues could be implemented in such a way that we stay in real arithmetic throughout the whole computation
* Stability considerations for `backward_subst!` (e.g. smallest value for which it is safe to divide). The aim is to have a similar implementation as in LAPACK.

# Major developments
During the time period, the project was jointly developed with my mentor and a lot of communication took place on Slack. I will list here the key PRs that I was a part of:

* Computing the QR decomposition through Givens rotations and using this in `implicit_restart!`: [#2](https://github.com/haampie/IRAM.jl/pull/2), [#4](https://github.com/haampie/IRAM.jl/pull/4)
* Locking of Ritz vectors: [#7](https://github.com/haampie/IRAM.jl/pull/7)
* QR step made implicit: [#10](https://github.com/haampie/IRAM.jl/pull/10)
* Added native Julia implementations of computing Schur factorization for a Hessenberg matrix and computing eigenvalues from the Schur form: [#28](https://github.com/haampie/IRAM.jl/pull/28), [#30](https://github.com/haampie/IRAM.jl/pull/30)
* Transform Schur form into eigenvectors: [#49](https://github.com/haampie/IRAM.jl/pull/49)

# How to use it?
An example of how to use the method:
```
julia> using IRAM, LinearAlgebra
# Generate a sparse matrix
julia> A = spdiagm(-1 => fill(-1.0, 99), 0 => fill(2.0, 100), 1 => fill(-1.001, 99));
# Compute Schur form of A
julia> schur_form,  = partial_schur(A, min = 12, max = 30, nev = 10, tol = 1e-10, maxiter = 20, which=LM());
julia> Q,R = schur_form.Q, schur_form.R;
julia> norm(A*Q - Q*R)
6.336794280593682e-11
# Compute eigenvalues and eigenvectors of A
julia> vals, vecs = schur_to_eigen(schur_form);
# Show that Ax = λx
julia> norm(A*vecs - vals'.*vecs)
6.335460143979987e-11
```

In the `partial_schur` function call, `A` is the `n` by `n` matrix whose eigenvalues and eigenvectors you want; `min` specifies the minimum dimension to which the Hessenberg matrix is reduced after `implicit_restart!`; `max` specifies the maximum dimension of the Hessenberg matrix at which point `iterate_arnoldi!` stops increasing the dimension of the Krylov subspace; `nev` specifies the minimum amount of eigenvalues the method gives you; `tol` specifies the criterion that determines when the eigenpairs are considered converged (in practice, smaller `tol` forces the eigenpairs to converge even more); `maxiter` specifies the maximum amount of restarts the method can perform before ending the iteration; and `which` is a `Target` structure and specifies which eigenvalues are desired (Largest magnitude, smallest real part, etc.).

The function call `partial_schur` returns a partial Schur decomposition of the matrix `A` (`AQ = QR`) where the upper triangular matrix R is of size `nev` by `nev` and the unitary matrix `Q` is of size `n` by `nev`. `schur_to_eigen` then computes the eigenvalues and eigenvectors of matrix `A` from the Schur decomposition.

The same method can be applied to solve the eigenfunctions of the Helmholtz equation. Below, two eigenfunctions are visualized in a mouth shaped domain with and open boundary condition on left and fixed boundary conditions on the rest of the sides. This is an elementary model of how sound waves can form in the mouth.
![My helpful screenshot]({{ "/assets/combined2.jpg" | absolute_url }})


# Benchmarks

The native `partial_schur` was benchmarked against ARPACK's `eigs`. However, it is difficult to compare benchmarks due to differences in the stopping criterion. In any case, it seems that even with a quite relaxed stopping criterion (`1e-5`) for natve `partial_schur`, it still has poorer performance compared to native `eigs`. However, an absolute criterion of `1e-5` might still be very strict when eigenvalues are very large. In an attempt to ignore the effect of differing stopping criterions, the time taken per one matrix-vector product is computed. 

__Generalized eigenvalue problem with sparse tridiagonal 1948 by 1948 matrix (smallest in magnitude)__

ARPACK's `eigs`:
```
BenchmarkTools.Trial:
  memory estimate:  23.03 MiB
  allocs estimate:  2214
  --------------
  minimum time:     83.047 ms (2.16% GC)
  median time:      98.686 ms (2.12% GC)
  mean time:        105.291 ms (4.37% GC)
  maximum time:     201.714 ms (45.27% GC)
  --------------
  samples:          48
  evals/sample:     1
```
Amount of matrix-vector products: 66
Mean time per matrix-vector product: 105.291 ms / 66 = 1.5953181818181819 ms

Native `partial_schur` (`tol = 1e-10`):
```
BenchmarkTools.Trial:
  memory estimate:  62.23 MiB
  allocs estimate:  5990
  --------------
  minimum time:     451.928 ms (0.45% GC)
  median time:      567.831 ms (0.47% GC)
  mean time:        575.171 ms (2.29% GC)
  maximum time:     748.566 ms (0.44% GC)
  --------------
  samples:          9
  evals/sample:     1
```
Amount of matrix-vector products: 473 
Mean time per matrix-vector product: 575.171 ms / 473 = 1.2160063424947147 ms

Native `partial_schur` (`tol = 1e-5`):
```
BenchmarkTools.Trial:
  memory estimate:  29.88 MiB
  allocs estimate:  2836
  --------------
  minimum time:     222.053 ms (0.34% GC)
  median time:      302.852 ms (0.48% GC)
  mean time:        328.230 ms (2.07% GC)
  maximum time:     600.037 ms (0.41% GC)
  --------------
  samples:          16
  evals/sample:     1
```
Amount of matrix-vector products: 231 
Mean time per matrix-vector product: 328.230 ms / 231 = 1.420909090909091 ms

Native `partial_schur` (`tol = 1e-1`):
```
BenchmarkTools.Trial:
  memory estimate:  4.34 MiB
  allocs estimate:  336
  --------------
  minimum time:     20.436 ms (0.00% GC)
  median time:      24.677 ms (0.00% GC)
  mean time:        29.240 ms (2.58% GC)
  maximum time:     112.312 ms (0.00% GC)
  --------------
  samples:          173
  evals/sample:     1
```
Amount of matrix-vector products: 231 
Mean time per matrix-vector product: 29.240 ms / 22 = 1.329090909090909 ms

# What's the benefit of all of this?
Benefits of implementing native replacement of `eigs`:
* Julia becomes a little more lightweight
* Native `eigs` can be applied to different array types
and different number types (BigFloats, DoubleFloats,
etc.)
* Native `eigs` is more readable and easier to maintain
than ARPACK's `eigs` that is written in Fortran 90
