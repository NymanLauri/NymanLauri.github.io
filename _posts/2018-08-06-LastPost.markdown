---
layout: post
title:  "Final Blog Post"
date:   2018-08-13
---

Now that we have finally arrived at the end of coding period, I want to give you an overview of the whole project: what was the initial motivation and plan; what has been done; and what still would need to be implemented.

# Code & documentation
Over the summer I worked on the [IRAM.jl](https://github.com/haampie/IRAM.jl) repository which aims at implementing ARPACK's `eigs` function in pure Julia for nonsymmetric matrices. The documentation of the method can be found [here](https://haampie.github.io/IRAM.jl/latest/).

# Original plan
The aim was to implement a native Julia solver for the eigenvalue problem

```math
Ax = λx.
```
In particular, the aim was to solve this problem for large and sparse, nonsymmetric matrices which would replace ARPACK's `eigs` function. This helps reduce the dependency of Julia language on ARPACK. I will provide benchmarks comparing the performance of the native `eigs` versus ARPACK's `eigs`. The aim at first was to get the method into the package IterativeSolvers.jl. However, it is also a possibility to just register the package separately.

# Initial project outline
* IRAM for real and complex arithmetics with QR iterations through Givens rotations. __DONE__
* Finding multiple eigenpairs which requires locking of converged Ritz vectors. __DONE__
* Adding search targets for eigenvalues. __DONE__
	- However, search targets are not implemented in the same way as in ARPACK. The method targets the eigenvalues that are largest in magnitude by default which has the highest convergence rate. If the user wants to target the eigenvalues that are the smallest in magnitude, they have to invert the problem as detailed in the [documentation](https://haampie.github.io/IRAM.jl/latest/theory/transformations.html#Targeting-eigenvalues-with-shift-and-invert-1).
* Adding support for generalized eigenvalue problems.
	- This functionality is not incorporated in the method either. Generalized eigenvalue problems can be solved if the user provides an input matrix that contains the effect of both matrices in the generalized eigenvalue problem. A detailed explanation can be found in the [documentation](https://haampie.github.io/IRAM.jl/latest/theory/transformations.html#Transformation-to-standard-form-for-non-singular-B-1). 

__In addition:__
* Making the implementation fully native which includes native Julia implementations for the `schurfact!` call and the `eigvals` call. __DONE__

After a few twists and turns, the search targets as well as support for generalized eigenvalue problems were implemented after all.

# What's left to do?

__Method-wise:__
* The criterion for convergence should be changed to how it is in `eigs`. `eigs` considers a Ritz value converged when its residual `||Av - vθ||` is smaller than the product of `tol` and `max(ɛ^{2/3}, |θ|)`, where `ɛ = eps(real(eltype(A)))/2` is LAPACK's machine epsilon. Native `partial_schur` considers the Ritz value converged only when the subdiagonal of the Hessenberg matrix in smaller than the user provided tolerance `tol`. 

__Functionality-wise:__
* `partial_schur` returns the partial Schur decomposition. What still needs to be done is to compute the eigenvectors and eigenvalues from the partial Schur decomposition. Computing eigenvalues is quite straightforward and is currently implemented in `eigvalues`. However, computing the eigenvectors requires taking into account some stability considerations that are not currently implemented. Hence, currently the implementation only returns the partial Schur decomposition.

__Performance-wise & stability-wise:__
* The code needs to be profiled in order to find areas of improvement. The method should closely follow the implementation of `eigs`, however sometimes native `partial_schur` seems to be much slower than `eigs`. In fact, in some cases the native `partial_schur` requires more iterations than `eigs` to find the eigenvalues of the same matrix which suggests that there are differences between the methods themselves. This could be partly caused by the differences in the stopping criterion.
* Currently known performance considerations
	- Temporary variables could be refactored in order to reduce the amount of allocations
	- In `backward_subst!`, the case when the method computes eigenvectors for conjugate eigenvalues could be implemented in such a way that we stay in real arithmetic throughout the whole computation
* Stability considerations for `backward_subst!` (e.g. smallest value for which it is safe to divide). The aim is to have a similar implementation as in LAPACK.

# Major developments
During the time period, the project was jointly developed with my mentor and a lot of communication took place on Slack. I will list here the key PRs that I was a part of:

* Computing the QR decomposition through Givens rotations and using this in `implicit_restart!`: [#2](https://github.com/haampie/IRAM.jl/pull/2), [#4](https://github.com/haampie/IRAM.jl/pull/4)
* Locking of Ritz vectors: [#7](https://github.com/haampie/IRAM.jl/pull/7)
* QR step made implicit: [#10](https://github.com/haampie/IRAM.jl/pull/10)
* Added native Julia implementations of computing Schur factorization for a Hessenberg matrix and computing eigenvalues from the Schur form: [#28](https://github.com/haampie/IRAM.jl/pull/28), [#30](https://github.com/haampie/IRAM.jl/pull/30)
* Transform Schur form into eigenvectors: [#49](https://github.com/haampie/IRAM.jl/pull/49)

# How to use it?
An example of how to use the method:
```
julia> using IRAM
julia> A = rand(100, 100)
julia> schur_form = partial_schur(A, min = 12, max = 30, nev = 10, tol = 1e-10, maxiter = 20, which=LM())
julia> Q,R = schur_form.Q, schur_form.R
julia> norm(A*Q - Q*R)
1.4782234971282938e-14
```

Here, `A` is the `n` by `n` matrix whose partial Schur form is to be computed; `min` specifies the minimum dimension to which the Hessenberg matrix is reduced after `implicit_restart!`; `max` specifies the maximum dimension of the Hessenberg matrix at which point `iterate_arnoldi!` stops increasing the dimension of the Krylov subspace; `nev` specifies the minimum amount of eigenvalues the method gives you; `tol` specifies the criterion that determines when the eigenpairs are considered converged (in practice, smaller `tol` forces the eigenpairs to converge even more); `maxiter` specifies the maximum amount of restarts the method can perform before ending the iteration; and `which` is a `Target` structure and specifies which eigenvalues are desired (smallest magnitude, smallest real part, etc.).

This returns a partial Schur decomposition of the matrix `A` (`AQ = QR`) where the upper triangular matrix R is of size `nev` by `nev` and the unitary matrix `Q` is of size `n` by `nev`.

# Benchmarks

The native `partial_schur` was benchmarked against ARPACK's `eigs`. However, it is difficult to compare benchmarks due to differences in the stopping criterion. In any case, it seems that even with a quite relaxed stopping criterion (`1e-5`) for natve `partial_schur`, it still has poorer performance compared to native `eigs`.

__Sparse tridiagonal 6000 by 6000 matrix (smallest in magnitude)__

ARPACK's `eigs`:
```
BenchmarkTools.Trial:
  memory estimate:  30.91 MiB
  allocs estimate:  1128
  --------------
  minimum time:     72.434 ms (1.07% GC)
  median time:      79.469 ms (1.66% GC)
  mean time:        79.370 ms (1.83% GC)
  maximum time:     92.863 ms (3.61% GC)
  --------------
  samples:          63
  evals/sample:     1
```

Native `partial_schur` (`tol = 1e-10`):
```
BenchmarkTools.Trial:
  memory estimate:  71.49 MiB
  allocs estimate:  3836
  --------------
  minimum time:     244.710 ms (0.62% GC)
  median time:      290.083 ms (0.51% GC)
  mean time:        305.355 ms (3.67% GC)
  maximum time:     397.500 ms (21.98% GC)
  --------------
  samples:          17
  evals/sample:     1
```

Native `partial_schur` (`tol = 1e-5`):
```
BenchmarkTools.Trial:
  memory estimate:  47.16 MiB
  allocs estimate:  2508
  --------------
  minimum time:     172.508 ms (0.79% GC)
  median time:      200.929 ms (0.64% GC)
  mean time:        206.289 ms (3.65% GC)
  maximum time:     277.538 ms (28.68% GC)
  --------------
  samples:          25
  evals/sample:     1
```

# What's the benefit of all of this?
Benefits of implementing native replacement of `eigs`:
* Julia becomes a little more lightweight
* Native `eigs` can be applied to different array types
and different number types (BigFloats, DoubleFloats,
etc.)
* Native `eigs` is more readable and easier to maintain
than ARPACK's `eigs` that is written in Fortran 90
